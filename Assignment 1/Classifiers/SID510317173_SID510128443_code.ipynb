{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 - Machine Learning and Data Mining: Assignment 1\n",
    "<div style=\"text-align: right\"> Due: Friday week 7 - Fri 24 Sep 2021 11:59PM </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "375753da-1c6c-4b02-986a-6e3b185a5869"
    }
   },
   "source": [
    "# 1. Summary\n",
    "The goal of this assignment is to build a classifier to classify some grayscale images of the size 28x28 into a set of categories. The dimension of the original data is large, so you need to be smart on which method you gonna use and perhaps perform a pre-processing step to reduce the amount of computation. Part of your marks will be a function of the performance of your classifier on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset description\n",
    "The dataset can be downloaded from Canvas. The dataset consists of a training set of 30,000 examples and a test set of 5,000 examples. They belong to 10 different categories. The validation set is not provided, but you can randomly pick a subset of the training set for validation. The labels of the first 2,000 test examples are given, you will analyse the performance of your proposed method by exploiting the 2,000 test examples. It is NOT allowed to use any examples from the test set for training; or it will be considered as cheating. The rest 3,000 labels of the test set are reserved for marking purpose. <br />\n",
    "Here are examples illustrating sample of the dataset (each class takes one row):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Dataset_image.jpg\" alt=\"DataSet\" title=\"DataSet\" width=\"450\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 classes in total:<br />\n",
    "0 T-shirt/Top<br />\n",
    "1 Trouser<br />\n",
    "2 Pullover<br />\n",
    "3 Dress<br />\n",
    "4 Coat<br />\n",
    "5 Sandal<br />\n",
    "6 Shirt<br />\n",
    "7 Sneaker<br />\n",
    "8 Bag<br />\n",
    "9 Ankle boot <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. How to load the data and make output prediciton\n",
    "There is a Input folder including 4 main files (which can be downloaded from Canvas):\n",
    "\n",
    "    1. images_training.h5 (30000 image samples for training)\n",
    "    \n",
    "    2. labels_training.h5 (30000 image lables for training)\n",
    "    \n",
    "    3. images_testing.h5 (5000 image samples for making prediction)\n",
    "    \n",
    "    4. labels_testing_2000.h5 (only 2000 image lables for testing, 3000 labels are not provided)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 How to load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the *hdf5* file and load the data into a numpy array. \n",
    "\n",
    "The **training data files are in the ./Input/train** and **testing data file are in ./Input/test**. <br /> Use the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then data would be a numpy array of the shape (30000, 784), and label would be a numpy array of the shape (30000, ).\n",
    "\n",
    "It is noted that the **labels_testing_2000** only contain **2000 samples** for your self-testing. The validation test for **fine-tuning parameters** should be splitted from the training test. We will evaluate your model on full 5000 samples which is not provided. The file **images_testing.h5** can be loaded in a similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images_training.h5', 'labels_training.h5']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "print(os.listdir(\"./Input/train\"))#setwd() dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784) (30000,) (5000, 784) (2000,)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('./Input/train/images_training.h5','r') as H: #784 = 28*28\n",
    "    data_train = np.copy(H['datatrain'])\n",
    "with h5py.File('./Input/train/labels_training.h5','r') as H:\n",
    "    label_train = np.copy(H['labeltrain'])\n",
    "with h5py.File('./Input/test/images_testing.h5','r') as H:\n",
    "    data_test = np.copy(H['datatest'])\n",
    "with h5py.File('./Input/test/labels_testing_2000.h5','r') as H:\n",
    "    label_test = np.copy(H['labeltest'])\n",
    "\n",
    "# using H['datatest'], H['labeltest'] for test dataset.\n",
    "print(data_train.shape,label_train.shape,data_test.shape,label_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1e4a01db-cd92-48f8-bdaa-21c39456cfcb"
    }
   },
   "source": [
    "# 4. Task description\n",
    "\n",
    "Your task is to determine / build a classifier for the given data set to classify images into categories and write a report. The score allocation is as follows:\n",
    "\n",
    "    * Code: max 65 points\n",
    "    * Report: max 35 points\n",
    "    \n",
    "Please refer to the rubric in Canvas for detailed marking scheme. The report and the code are to be submitted in Canvas by the due date.<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Code\n",
    "### The code must clearly show :\n",
    "    1. Pre-process data\n",
    "    1. Details of your implementation for each algorithm\n",
    "    2. Fine-tune hyper-parameters for each algorithm and running time\n",
    "    3. The comparison result between algorithms\n",
    "    4. Hardware and software specifications of the computer that you used for performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to have at least one pre-process techique before you can apply the classification algorithms. One of pre-process techique is using **Normalisation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data_train #from above whole data, for training\n",
    "y = label_train #from above whole label, for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_train, label_train, test_size=0.3)\n",
    "\n",
    "X_predict = data_test[:2000] #for validation 2000 data\n",
    "#X_output = data_test[2000:]#for result and output h5 files 3000 data\n",
    "X_output = data_test\n",
    "\n",
    "#X_train = data_train\n",
    "#y_train = label_train\n",
    "#y_test = label_test\n",
    "#X_train = X_train.reshape(30000, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the Data (mean = 0 and variance = 1) y=mx+b\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_norm = scaler.transform(X) \n",
    "X_train_norm = scaler.transform(X_train) \n",
    "X_test_norm = scaler.transform(X_test) \n",
    "X_predict_norm =  scaler.transform(X_predict) \n",
    "X_output_norm =  scaler.transform(X_output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PCA Analysis \n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "X_pca = pca.fit_transform(X_norm)\n",
    "X_train = pca.transform(X_train_norm)\n",
    "X_test = pca.transform(X_test_norm)\n",
    "X_predict = pca.transform(X_predict_norm)\n",
    "X_output = pca.transform(X_output_norm) #784 vairble per data\n",
    "\n",
    "#plot out the pca analysis graph\n",
    "f = plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose 188 as the elbow point where the data explain around 95% of the total variance\n",
    "pca = PCA(n_components=0.95) #95% total variance\n",
    "X_pca = pca.fit_transform(X) #188 variable per data\n",
    "X_train_pca = pca.transform(X_train_norm)\n",
    "X_test_pca = pca.transform(X_test_norm)\n",
    "X_predict_pca = pca.transform(X_predict_norm)\n",
    "X_output_pca = pca.transform(X_output_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Classification algorithms with 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now apply multiple classifiers to the pre-processed dataset. You have to implement at least 3 classifiers in particular:\n",
    "\n",
    "    * Nearest Neighbor\n",
    "    * Logistic Regression\n",
    "    * NaÃ¯ve Bayes \n",
    "    * Decision Tree\n",
    "    * Bagging\n",
    "    * Ada Boost\n",
    "    * SVM\n",
    "    \n",
    "You need to evaluate the performance of these classifiers using 10-fold cross-validation. For binary classifiers, we can use those classifiers for the data which has more than 2 labels using the one-vs-rest method. The implementation can use sklearn, or can be implemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.model_selection import cross_val_score\\nimport timeit\\n\\nstart = timeit.default_timer()\\n\\nknn = KNeighborsClassifier()\\nknn.fit(X_train_norm, y_train) #norm pca\\ny_pred_knn = knn.predict(X_test_norm)\\nprint(\"Original data - knn accuracy: {:.3f}\".format(accuracy_score(y_test,y_pred_knn)))\\n#normal_scores = cross_val_score(knn, X_norm, y, cv=10, scoring=\\'accuracy\\')\\n\\nknn = KNeighborsClassifier()\\nknn.fit(X_train_pca, y_train)\\ny_pred_pca_knn = knn.predict(X_test_pca)\\nprint(\"Reduced data - knn accuracy: {:.3f}\".format(accuracy_score(y_test,y_pred_pca_knn)))\\npca_scores = cross_val_score(knn, X_pca, y, cv=10, scoring=\\'accuracy\\')\\n\\n#10-fold vialidation\\n#print(\"orginal data - knn accuracy of the 10-fold validation: \",normal_scores)\\nprint(\"Reduced data - knn accuracy of the 10-fold validation: \",pca_scores)\\n\\nstop = timeit.default_timer()\\nprint(\\'Time: \\', stop - start) \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knn model data all comes from train folder\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_norm, y_train) #norm pca\n",
    "y_pred_knn = knn.predict(X_test_norm)\n",
    "print(\"Original data - knn accuracy: {:.3f}\".format(accuracy_score(y_test,y_pred_knn)))\n",
    "#normal_scores = cross_val_score(knn, X_norm, y, cv=10, scoring='accuracy')\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_pca, y_train)\n",
    "y_pred_pca_knn = knn.predict(X_test_pca)\n",
    "print(\"Reduced data - knn accuracy: {:.3f}\".format(accuracy_score(y_test,y_pred_pca_knn)))\n",
    "pca_scores = cross_val_score(knn, X_pca, y, cv=10, scoring='accuracy')\n",
    "\n",
    "#10-fold vialidation\n",
    "#print(\"orginal data - knn accuracy of the 10-fold validation: \",normal_scores)\n",
    "print(\"Reduced data - knn accuracy of the 10-fold validation: \",pca_scores)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.model_selection import cross_val_score\\nimport timeit\\n\\nstart = timeit.default_timer()\\n\\ntree = DecisionTreeClassifier(criterion=\\'entropy\\')\\ntree.fit(X_train_norm, y_train)\\ny_pred_tree = tree.predict(X_test_norm)\\nprint(\"Original data - tree accuracy: {:.3f}\".format(accuracy_score(y_test,y_pred_tree)))\\n#normal_scores = cross_val_score(tree, X_norm, y, cv=10, scoring=\\'accuracy\\')\\n\\ntree = DecisionTreeClassifier(criterion=\\'entropy\\')\\ntree.fit(X_train_pca, y_train)\\ny_pred_pca_tree = tree.predict(X_test_pca)\\nprint(\"Reduced data - tree accuracy: {:.3f}\".format(accuracy_score(y_test,y_pred_pca_tree)))\\npca_scores = cross_val_score(tree, X_pca, y, cv=10, scoring=\\'accuracy\\')\\n\\n#10-fold vialidation\\n#print(\"orginal data - Decision Tree accuracy of the 10-fold validation: \",normal_scores)\\nprint(\"Reduced data - Decision Tree accuracy of the 10-fold validation: \",pca_scores)\\n\\nstop = timeit.default_timer()\\nprint(\\'Time: \\', stop - start) \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision Tree\n",
    "\"\"\"\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy')\n",
    "tree.fit(X_train_norm, y_train)\n",
    "y_pred_tree = tree.predict(X_test_norm)\n",
    "print(\"Original data - tree accuracy: {:.3f}\".format(accuracy_score(y_test,y_pred_tree)))\n",
    "#normal_scores = cross_val_score(tree, X_norm, y, cv=10, scoring='accuracy')\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy')\n",
    "tree.fit(X_train_pca, y_train)\n",
    "y_pred_pca_tree = tree.predict(X_test_pca)\n",
    "print(\"Reduced data - tree accuracy: {:.3f}\".format(accuracy_score(y_test,y_pred_pca_tree)))\n",
    "pca_scores = cross_val_score(tree, X_pca, y, cv=10, scoring='accuracy')\n",
    "\n",
    "#10-fold vialidation\n",
    "#print(\"orginal data - Decision Tree accuracy of the 10-fold validation: \",normal_scores)\n",
    "print(\"Reduced data - Decision Tree accuracy of the 10-fold validation: \",pca_scores)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data - SVM accuracy: 0.878\n",
      "Reduced data - SVM accuracy: 0.885\n",
      "Reduced data - Decision Tree accuracy of the 10-fold validation:  [0.87833333 0.88766667 0.87833333 0.883      0.89766667 0.888\n",
      " 0.89166667 0.88133333 0.893      0.9       ]\n",
      "Time:  310.8170769\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_norm, y_train)\n",
    "y_pred_svm = svm.predict(X_test_norm)\n",
    "print(\"Original data - SVM accuracy: {:.3f}\".format(accuracy_score(y_test,y_pred_svm)))\n",
    "#normal_scores = cross_val_score(svm, X_norm, y, cv=10, scoring='accuracy')\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_pca, y_train)\n",
    "y_pred_pca_svm = svm.predict(X_test_pca)\n",
    "print(\"Reduced data - SVM accuracy: {:.3f}\".format(accuracy_score(y_test,y_pred_pca_svm)))\n",
    "pca_scores = cross_val_score(svm, X_pca, y, cv=10, scoring='accuracy')\n",
    "\n",
    "#10-fold vialidation\n",
    "\n",
    "#print(\"orginal data - Decision Tree accuracy of the 10-fold validation: \",normal_scores)\n",
    "print(\"Reduced data - Decision Tree accuracy of the 10-fold validation: \",pca_scores)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each classifiers we would like to find the best parameters using grid search with 10-fold stratified cross validation.\n",
    "\n",
    "- SVM\n",
    "- Decision Tree\n",
    "- Logistic \n",
    "- KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import GridSearchCV\\nparam_grid = {\\n    \\'n_neighbors\\':[2,4,6,8,10,12,14,16],\\n}\\nknn_grid_search = GridSearchCV(knn,param_grid,cv=10,return_train_score=True,n_jobs=-1)\\nknn_grid_search.fit(X_pca,y)\\n#y_pred_grid = grid_search.predict(X_test_norm)\\n\\nprint(\"Grid search best accuraccy on ttest set:{:3f}\".format(knn_grid_search.score(X_pca, y)))\\nprint(\"Best parameters:{}\".format(knn_grid_search.best_params_))\\nprint(\"Best cv score:{}\".format(knn_grid_search.best_score_))\\nprint(\"Best estimator:\\n{}\".format(knn_grid_search.best_estimator_))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knn best n = 8\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_neighbors':[2,4,6,8,10,12,14,16],\n",
    "}\n",
    "knn_grid_search = GridSearchCV(knn,param_grid,cv=10,return_train_score=True,n_jobs=-1)\n",
    "knn_grid_search.fit(X_pca,y)\n",
    "#y_pred_grid = grid_search.predict(X_test_norm)\n",
    "\n",
    "print(\"Grid search best accuraccy on ttest set:{:3f}\".format(knn_grid_search.score(X_pca, y)))\n",
    "print(\"Best parameters:{}\".format(knn_grid_search.best_params_))\n",
    "print(\"Best cv score:{}\".format(knn_grid_search.best_score_))\n",
    "print(\"Best estimator:\\n{}\".format(knn_grid_search.best_estimator_))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor mean_score, rank, params in zip(knn_grid_search.cv_results_[\\'mean_test_score\\'],\\n                                   knn_grid_search.cv_results_[\\'rank_test_score\\'],\\n                                   knn_grid_search.cv_results_[\\'params\\']):\\n    print(\"Mean test score is {:0.3f}, and rank is {:0.0f} for {}\".format(\\n            mean_score, rank, params))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for mean_score, rank, params in zip(knn_grid_search.cv_results_['mean_test_score'],\n",
    "                                   knn_grid_search.cv_results_['rank_test_score'],\n",
    "                                   knn_grid_search.cv_results_['params']):\n",
    "    print(\"Mean test score is {:0.3f}, and rank is {:0.0f} for {}\".format(\n",
    "            mean_score, rank, params))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparam_grid = {\\n    \\'n_neighbors\\':[2,4,6,8,10,12,14,16],\\n}\\nscores = knn_grid_search.cv_results_[\\'mean_test_score\\']\\nplt.plot(param_grid[\\'n_neighbors\\'],scores,color=\\'blue\\', linestyle=\\'solid\\', marker=\\'o\\',\\n     markerfacecolor=\\'red\\', markersize=7)\\n\\nplt.title(\"KNN - Grid Search Scores\", fontsize=16)\\nplt.xlabel(\\'n_neighbors\\')\\nplt.ylabel(\\'Mean test score\\')\\nplt.grid(True)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "param_grid = {\n",
    "    'n_neighbors':[2,4,6,8,10,12,14,16],\n",
    "}\n",
    "scores = knn_grid_search.cv_results_['mean_test_score']\n",
    "plt.plot(param_grid['n_neighbors'],scores,color='blue', linestyle='solid', marker='o',\n",
    "     markerfacecolor='red', markersize=7)\n",
    "\n",
    "plt.title(\"KNN - Grid Search Scores\", fontsize=16)\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('Mean test score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import GridSearchCV\\nparam_grid = {\\n    \\'criterion\\':[\\'entropy\\'],\\n    \\'max_depth\\': [3,7,11,15,19,23],\\n    \\'splitter\\':[\\'best\\',\\'random\\']\\n}\\ntree_grid_search = GridSearchCV(tree,param_grid,cv=10,return_train_score=True,n_jobs=-1)\\ntree_grid_search.fit(X_pca,y)\\n#y_pred_grid = grid_search.predict(X_test_norm)\\nprint(\"Grid search best accuraccy on ttest set:{:3f}\".format(tree_grid_search.score(X_pca, y)))\\nprint(\"Best parameters:{}\".format(tree_grid_search.best_params_))\\nprint(\"Best cv score:{}\".format(tree_grid_search.best_score_))\\nprint(\"Best estimator:\\n{}\".format(tree_grid_search.best_estimator_))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decision tree\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion':['entropy'],\n",
    "    'max_depth': [3,7,11,15,19,23],\n",
    "    'splitter':['best','random']\n",
    "}\n",
    "tree_grid_search = GridSearchCV(tree,param_grid,cv=10,return_train_score=True,n_jobs=-1)\n",
    "tree_grid_search.fit(X_pca,y)\n",
    "#y_pred_grid = grid_search.predict(X_test_norm)\n",
    "print(\"Grid search best accuraccy on ttest set:{:3f}\".format(tree_grid_search.score(X_pca, y)))\n",
    "print(\"Best parameters:{}\".format(tree_grid_search.best_params_))\n",
    "print(\"Best cv score:{}\".format(tree_grid_search.best_score_))\n",
    "print(\"Best estimator:\\n{}\".format(tree_grid_search.best_estimator_))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor mean_score, rank, params in zip(tree_grid_search.cv_results_[\\'mean_test_score\\'],\\n                                   tree_grid_search.cv_results_[\\'rank_test_score\\'],\\n                                   tree_grid_search.cv_results_[\\'params\\']):\\n    print(\"Mean test score is {:0.3f}, and rank is {:0.0f}) for {}\".format(\\n            mean_score, rank, params))\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for mean_score, rank, params in zip(tree_grid_search.cv_results_['mean_test_score'],\n",
    "                                   tree_grid_search.cv_results_['rank_test_score'],\n",
    "                                   tree_grid_search.cv_results_['params']):\n",
    "    print(\"Mean test score is {:0.3f}, and rank is {:0.0f}) for {}\".format(\n",
    "            mean_score, rank, params))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparam_grid = {\\n    \\'criterion\\':[\\'entropy\\'],\\n    \\'max_depth\\': [3,7,11,15,19,23],\\n    \\'splitter\\':[\\'best\\',\\'random\\']\\n}\\n\\nscores = tree_grid_search.cv_results_[\\'mean_test_score\\']\\nbest,random = scores[::2], scores[1::2]\\nscores = [best,random]\\n\\nfor ind, i in enumerate(param_grid[\\'splitter\\']):\\n    plt.plot(param_grid[\\'max_depth\\'], scores[ind], label=\\'criterion: \\' + str(i))\\nplt.legend()\\nplt.title(\"Decision Tree - Grid Search Scores\", fontsize=16)\\nplt.xlabel(\\'Max Depth\\')\\nplt.ylabel(\\'Mean test score\\')\\nplt.grid(True)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "param_grid = {\n",
    "    'criterion':['entropy'],\n",
    "    'max_depth': [3,7,11,15,19,23],\n",
    "    'splitter':['best','random']\n",
    "}\n",
    "\n",
    "scores = tree_grid_search.cv_results_['mean_test_score']\n",
    "best,random = scores[::2], scores[1::2]\n",
    "scores = [best,random]\n",
    "\n",
    "for ind, i in enumerate(param_grid['splitter']):\n",
    "    plt.plot(param_grid['max_depth'], scores[ind], label='criterion: ' + str(i))\n",
    "plt.legend()\n",
    "plt.title(\"Decision Tree - Grid Search Scores\", fontsize=16)\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Mean test score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import GridSearchCV\\nparam_grid = {\\n    \\'kernel\\': [\\'rbf\\',\\'linear\\'],\\n    \\'C\\':[5,7,9,11,13],\\n    \\'gamma\\':[\\'scale\\']\\n}\\n#gamma = 1 / (n_features * X.var())\\nsvm_grid_search = GridSearchCV(svm,param_grid,cv=10,return_train_score=True,n_jobs=-1)\\nsvm_grid_search.fit(X_pca,y)#X_pca 30000, 188 y:30000,(1)\\n#y_pred_grid = grid_search.predict(X_test_norm)\\nprint(\"Grid search best accuraccy on ttest set:{:3f}\".format(svm_grid_search.score(X_pca, y)))\\nprint(\"Best parameters:{}\".format(svm_grid_search.best_params_))\\nprint(\"Best cv score:{}\".format(svm_grid_search.best_score_))\\nprint(\"Best estimator:\\n{}\".format(svm_grid_search.best_estimator_))\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#svm\n",
    "#best accuracy is 88.6%, Best parameters:{'C': 6, 'gamma': 'scale', 'kernel': 'rbf'}\n",
    "#gamma = 0.03,0.035,0.04,0.045,0.05 -> 0.035\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'kernel': ['rbf','linear'],\n",
    "    'C':[5,7,9,11,13],\n",
    "    'gamma':['scale']\n",
    "}\n",
    "#gamma = 1 / (n_features * X.var())\n",
    "svm_grid_search = GridSearchCV(svm,param_grid,cv=10,return_train_score=True,n_jobs=-1)\n",
    "svm_grid_search.fit(X_pca,y)#X_pca 30000, 188 y:30000,(1)\n",
    "#y_pred_grid = grid_search.predict(X_test_norm)\n",
    "print(\"Grid search best accuraccy on ttest set:{:3f}\".format(svm_grid_search.score(X_pca, y)))\n",
    "print(\"Best parameters:{}\".format(svm_grid_search.best_params_))\n",
    "print(\"Best cv score:{}\".format(svm_grid_search.best_score_))\n",
    "print(\"Best estimator:\\n{}\".format(svm_grid_search.best_estimator_))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor mean_score, rank, params in zip(svm_grid_search.cv_results_[\\'mean_test_score\\'],\\n                                   svm_grid_search.cv_results_[\\'rank_test_score\\'],\\n                                   svm_grid_search.cv_results_[\\'params\\']):\\n    print(\"Mean test score is {:0.3f}, and rank is {:0.0f} for {}\".format(\\n            mean_score, rank, params))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for mean_score, rank, params in zip(svm_grid_search.cv_results_['mean_test_score'],\n",
    "                                   svm_grid_search.cv_results_['rank_test_score'],\n",
    "                                   svm_grid_search.cv_results_['params']):\n",
    "    print(\"Mean test score is {:0.3f}, and rank is {:0.0f} for {}\".format(\n",
    "            mean_score, rank, params))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparam_grid = {\\n    \\'kernel\\': [\\'rbf\\',\\'linear\\'],\\n    \\'C\\':[5,7,9,11,13],\\n    \\'gamma\\':[\\'scale\\']\\n}\\nscores = svm_grid_search.cv_results_[\\'mean_test_score\\']\\nrbf,linear = scores[::2], scores[1::2]\\nscores = [rbf,linear]\\n#plt.plot(param_grid[\\'max_depth\\'],scores,color=\\'blue\\', linestyle=\\'solid\\', marker=\\'o\\',markerfacecolor=\\'red\\', markersize=7)\\n\\nfor ind, i in enumerate(param_grid[\\'kernel\\']):\\n    plt.plot(param_grid[\\'C\\'], scores[ind], label=\\'criterion: \\' + str(i))\\nplt.legend()\\nplt.title(\"SVM - Grid Search Scores\", fontsize=16)\\nplt.xlabel(\\'C Value\\')\\nplt.ylabel(\\'Mean test score\\')\\nplt.grid(True)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "param_grid = {\n",
    "    'kernel': ['rbf','linear'],\n",
    "    'C':[5,7,9,11,13],\n",
    "    'gamma':['scale']\n",
    "}\n",
    "scores = svm_grid_search.cv_results_['mean_test_score']\n",
    "rbf,linear = scores[::2], scores[1::2]\n",
    "scores = [rbf,linear]\n",
    "#plt.plot(param_grid['max_depth'],scores,color='blue', linestyle='solid', marker='o',markerfacecolor='red', markersize=7)\n",
    "\n",
    "for ind, i in enumerate(param_grid['kernel']):\n",
    "    plt.plot(param_grid['C'], scores[ind], label='criterion: ' + str(i))\n",
    "plt.legend()\n",
    "plt.title(\"SVM - Grid Search Scores\", fontsize=16)\n",
    "plt.xlabel('C Value')\n",
    "plt.ylabel('Mean test score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Classifier comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the best parameter for each algorithm, we would like to make comparisons between all classifiers using their own best hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.model_selection import cross_val_score\\nimport timeit\\n\\nstart = timeit.default_timer()\\n\\nknn = KNeighborsClassifier(n_neighbors=8)\\nknn.fit(X_train_pca, y_train)\\n\\ny_pred_train_knn = knn.predict(X_pca)\\nprint(\"Training Set - knn accuracy: {:.3f}\".format(accuracy_score(label_train,y_pred_train_knn)))\\n\\ny_pred_test_knn = knn.predict(X_predict_pca)\\nprint(\"Testing Set - knn accuracy: {:.3f}\".format(accuracy_score(label_test,y_pred_test_knn)))\\n\\nstop = timeit.default_timer()\\nprint(\\'Time: \\', stop - start)  \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knn model\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "knn.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred_train_knn = knn.predict(X_pca)\n",
    "print(\"Training Set - knn accuracy: {:.3f}\".format(accuracy_score(label_train,y_pred_train_knn)))\n",
    "\n",
    "y_pred_test_knn = knn.predict(X_predict_pca)\n",
    "print(\"Testing Set - knn accuracy: {:.3f}\".format(accuracy_score(label_test,y_pred_test_knn)))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.model_selection import cross_val_score\\nimport timeit\\n\\nstart = timeit.default_timer()\\n\\ntree = DecisionTreeClassifier(criterion=\\'entropy\\', max_depth=11)\\ntree.fit(X_train_pca, y_train)\\n\\ny_pred_train_tree = tree.predict(X_pca)\\nprint(\"Training Set - tree accuracy: {:.3f}\".format(accuracy_score(label_train,y_pred_train_tree)))\\n\\ny_pred_test_tree = tree.predict(X_predict_pca)\\nprint(\"Testing Set - tree accuracy: {:.3f}\".format(accuracy_score(label_test,y_pred_test_tree)))\\n\\nstop = timeit.default_timer()\\nprint(\\'Time: \\', stop - start)  \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision Tree\n",
    "\"\"\"\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=11)\n",
    "tree.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred_train_tree = tree.predict(X_pca)\n",
    "print(\"Training Set - tree accuracy: {:.3f}\".format(accuracy_score(label_train,y_pred_train_tree)))\n",
    "\n",
    "y_pred_test_tree = tree.predict(X_predict_pca)\n",
    "print(\"Testing Set - tree accuracy: {:.3f}\".format(accuracy_score(label_test,y_pred_test_tree)))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set - svm accuracy: 0.952\n",
      "Testing Set - svm accuracy: 0.880\n",
      "Time:  51.65474329999995\n"
     ]
    }
   ],
   "source": [
    "#SVM, c=9,gamma=scale -> 88%]\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "svm = SVC(kernel='rbf',C=9,gamma='scale')\n",
    "svm.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred_train_svm = svm.predict(X_pca)\n",
    "print(\"Training Set - svm accuracy: {:.3f}\".format(accuracy_score(label_train,y_pred_train_svm)))\n",
    "\n",
    "y_pred_test_svm = svm.predict(X_predict_pca)\n",
    "print(\"Testing Set - svm accuracy: {:.3f}\".format(accuracy_score(label_test,y_pred_test_svm)))\n",
    "\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data - knn accuracy: 0.873\n",
    "Reduced data - knn accuracy: 0.834\n",
    "\n",
    "Original data - tree accuracy: 0.853\n",
    "Reduced data - tree accuracy: 0.756\n",
    "\n",
    "Original data - svm accuracy: 0.951\n",
    "Reduced data - svm accuracy: 0.883\n",
    "\n",
    "the pca svm model shows the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='rbf',C=9,gamma='scale')\n",
    "svm.fit(X_train_pca, y_train)\n",
    "y_pred_pca_svm = svm.predict(X_output_pca)\n",
    "\n",
    "#y_output = svm.predict(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\nX_output = data_test[2000:]\\nX_output = X_output.reshape((X_output.shape[0], 28, 28))\\nfor i in range(20):\\n    plt.imshow(X_output[i], cmap=plt.get_cmap(\\'gray\\'))\\n    plt.title(\"class \" + str(y_pred_pca_svm[i]))\\n    plt.show()\\n    \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "X_output = data_test[2000:]\n",
    "X_output = X_output.reshape((X_output.shape[0], 28, 28))\n",
    "for i in range(20):\n",
    "    plt.imshow(X_output[i], cmap=plt.get_cmap('gray'))\n",
    "    plt.title(\"class \" + str(y_pred_pca_svm[i]))\n",
    "    plt.show()\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# assume output is the predicted labels from classifiers\n",
    "# (5000,) .H5 file\n",
    "with h5py.File('Output/predicted_labels.h5','w') as H:\n",
    "    H.create_dataset('Output',data=y_pred_pca_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The report must clearly show:\n",
    "    1. Details of your classifiers using for assignment 1\n",
    "    2. The predicted results from your classifier on test examples\n",
    "    3. Results comparison and discussion\n",
    "    4. Following the format in rubric : Introduction -> Methods -> Experiments result and discussion -> Conclusion\n",
    "    5. The maximum length of the report is 10 (including references)\n",
    "    6. Clearly provide instructions on how to run your code in the Appendix section of your report\n",
    "    7. Detail of student including ID, name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Instructions to hand in the assignment\n",
    "\n",
    "### Go to Canvas -> Assignments -> \"Assignment 1\" and submit 3 files only: the report and the code files.\n",
    "\n",
    "1) Report (a .pdf file).\n",
    "\n",
    "2) Code (2 files include: a .ipynb file and a PDF file). PDF is exported from .ipynb file for plagiarism check.\n",
    "The code must be able to be run with the following folder structure:\n",
    "\n",
    "    - Classifiers (the root folder): Your .ipynb file containing Python code will be placed on this folder when we test and run your code. The PDF file is generated from .ipynb file (File => Save as PDF file)\n",
    "    \n",
    "    - Input (a sub-folder under Algorithm): We will copy the dataset into this Input folder when we run your code. Please make sure your code is able to read the dataset from this Input folder.\n",
    "    \n",
    "    - Output (a sub-folder under Algorithm): Your code must be able to generate a prediction file named âpredicted_labels.h5â to be saved in this Output folder. The prediction file should contain predicted labels of the test dataset. We will use your prediction output file for grading purpose.\n",
    "\n",
    "Since this is a individual work, each student needs to submit all the files which must be named with student ID numbers following format e.g. **SIDxxxx_report.pdf**,  **SIDxxxx_code.ipynb**, **SIDxxxx_code.ipynb.pdf**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A penalty of MINUS 5 percent (-5%) for each day after the due date. \n",
    "The maximum delay for assignment submission is 5 (five) days, after which assignment will not be accepted.\n",
    "\n",
    "**You should upload your assignment at least half a day or one day prior to the submission deadline to avoid network congestion**.\n",
    "\n",
    "Canvas may not be able to handle a large number of submission happening at the same time. If you submit your assignment at a time close to the deadline, a submission error may occur causing your submission to be considered late. Penalty will be applied to late submission regardless of issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All files required for assignment 1 can be downloaded from Canvas -> Assignments -> Assignment 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Academic honesty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read the University policy on Academic Honesty very carefully: \n",
    "https://sydney.edu.au/students/academic-integrity.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plagiarism (copying from another student, website or other sources), making your work available to another student to copy, engaging another person to complete the assignments instead of you (for payment or not) are all examples of academic dishonesty. Note that when there is copying between students, both students are penalised â the student who copies and the student who makes his/her work available for copying. The University penalties are severe and include: \n",
    "\n",
    "    * a permanent record of academic dishonesty on your student file, \n",
    "    * mark deduction, ranging from 0 for the assignment to Fail for the course\n",
    "    * expulsion from the University and cancelling of your student visa. \n",
    "\n",
    "In addition, the Australian Government passed a new legislation last year (Prohibiting Academic Cheating Services Bill) that makes it a criminal offence to provide or advertise academic cheating services - the provision or undertaking of work for students which forms a substantial part of a studentâs assessment task. Do not confuse legitimate co-operation and cheating! You can discuss the assignment with another student, this is a legitimate collaboration, but you cannot complete the assignment together â this is an individual assignment and everyone must write their own code."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
